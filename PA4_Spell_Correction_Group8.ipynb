{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3m9JjeM5U5"
   },
   "source": [
    "# **Programming Assessment \\#4**\n",
    "\n",
    "Names: Alyanna Abalos, Loben Tipan\n",
    "\n",
    "More information on the assessment is found in our Canvas course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtmCAZwNoeU"
   },
   "source": [
    "# **Load Data**\n",
    "\n",
    "*While you don't have to separate your code into blocks, it might be easier if you separated loading your data from actually implementation of your code. Consider placing all loading of data into the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CbvxU2oTM4IV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('count_1edit.txt', 'r', encoding='ISO-8859-1') as file:\n",
    "    data_count_1edit = file.read()\n",
    "\n",
    "with open('spell_errors.txt', 'r', encoding='ISO-8859-1') as file:\n",
    "    data_spell_errors = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YCZLi-N0uR"
   },
   "source": [
    "# **Error Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('e', 'i'), 0.3411458333333333), (('a', 'e'), 0.24160316116285635), (('i', 'e'), 0.21761219305673157), (('e', 'a'), 0.2843583902809415), (('a', 'i'), 0.20796130952380953)] \n",
      "\n",
      "\n",
      "[('raining', ['rainning', 'raning']), ('writings', ['writtings']), ('disparagingly', ['disparingly']), ('yellow', ['yello']), ('four', ['forer', 'fours', 'fuore', 'fore*5', 'for*4'])]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "error_model = {}\n",
    "for line in data_count_1edit.splitlines():\n",
    "    parts = line.split('|')\n",
    "    w = parts[0].strip() \n",
    "    \n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "\n",
    "    c_split = parts[1].split()\n",
    "    if len(c_split) < 2:\n",
    "        continue\n",
    "\n",
    "    c = c_split[0]\n",
    "    count = int(c_split[1])\n",
    "    \n",
    "    error_model[(w, c)] = count\n",
    "\n",
    "total_counts_for_correct = defaultdict(int)\n",
    "for (corrupted, correct), count in error_model.items():\n",
    "    total_counts_for_correct[correct] += count\n",
    "\n",
    "for (corrupted, correct) in error_model:\n",
    "    error_model[(corrupted, correct)] /= total_counts_for_correct[correct]\n",
    "\n",
    "spell_errors = {}\n",
    "for line in data_spell_errors.splitlines():\n",
    "    correct_word, misspelled_words_part = line.split(\":\")\n",
    "    misspelled_words = [word.strip() for word in misspelled_words_part.split(',')]\n",
    "    spell_errors[correct_word.strip()] = misspelled_words\n",
    "\n",
    "print(list(error_model.items())[:5], \"\\n\\n\")\n",
    "print(list(spell_errors.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Language Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\loben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from collections import Counter\n",
    "\n",
    "def build_language_model():\n",
    "    words = gutenberg.words()\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    word_freq = Counter(words)\n",
    "    total_words = sum(word_freq.values())\n",
    "    language_model = {word: freq / total_words for word, freq in word_freq.items()}\n",
    "    return language_model\n",
    "\n",
    "language_model = build_language_model()\n",
    "vocabulary = set(language_model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Candidate Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(word, vocabulary):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(len(word)):\n",
    "        candidate = word[:i] + word[i+1:]\n",
    "        if candidate in vocabulary:\n",
    "            candidates.append((candidate, \"del\"))\n",
    "    \n",
    "    for i in range(len(word) + 1):\n",
    "        for letter in alphabet:\n",
    "            candidate = word[:i] + letter + word[i:]\n",
    "            if candidate in vocabulary:\n",
    "                candidates.append((candidate, \"ins\"))\n",
    "    \n",
    "    for i in range(len(word)):\n",
    "        for letter in alphabet:\n",
    "            if word[i] != letter:  # Ensure we're changing the character\n",
    "                candidate = word[:i] + letter + word[i+1:]\n",
    "                if candidate in vocabulary:\n",
    "                    candidates.append((candidate, \"sub\"))\n",
    "    \n",
    "    for i in range(len(word) - 1):\n",
    "        candidate = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
    "        if candidate in vocabulary:\n",
    "            candidates.append((candidate, \"tra\"))\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "\n",
    "word = \"mohter\"\n",
    "candidates = generate_candidates(word, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Edit Identifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_edit(word, candidate, operation):\n",
    "    \"\"\"Return the character edit operation between word and candidate.\"\"\"\n",
    "    if operation == \"sub\":\n",
    "        for i in range(len(word)):\n",
    "            if word[i] != candidate[i]:\n",
    "                return \"{}|{}\".format(word[i], candidate[i])\n",
    "    elif operation == \"ins\":\n",
    "        for i in range(len(word)):\n",
    "            if i == len(candidate) or word[i] != candidate[i]:\n",
    "                return f\"|{candidate[i]}\"\n",
    "        return f\"|{candidate[-1]}\"\n",
    "    elif operation == \"del\":\n",
    "        for i in range(len(candidate)):\n",
    "            if i == len(word) or word[i] != candidate[i]:\n",
    "                return f\"{word[i]}|\"\n",
    "        return f\"{word[-1]}|\"\n",
    "    elif operation == \"tra\":\n",
    "        for i in range(len(word) - 1):\n",
    "            if (word[i] != candidate[i]) and (word[i] == candidate[i+1]) and (word[i+1] == candidate[i]):\n",
    "                return \"{}{}|{}{}\".format(word[i], word[i+1], candidate[i], candidate[i+1])\n",
    "    return \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_correction_probabilities_log(word, candidates, error_model, language_model):\n",
    "    probabilities = []\n",
    "    SMOOTHING_FACTOR = 0.000001  # Define a small smoothing factor\n",
    "\n",
    "    for candidate, op in candidates:\n",
    "        # Determine character edit\n",
    "        char_edit = get_character_edit(word, candidate, op)\n",
    "        \n",
    "        # Get log error probability log P(c|w) with smoothing for unseen edits\n",
    "        log_p_c_given_w = math.log(error_model.get((char_edit), SMOOTHING_FACTOR))\n",
    "        \n",
    "        # Get log language model probability log P(w) with default value of log(0) if not found\n",
    "        log_p_w = math.log(language_model.get(candidate, SMOOTHING_FACTOR))\n",
    "        \n",
    "        # Compute combined log probability log P(w|c) = log P(c|w) + log P(w)\n",
    "        log_p_w_given_c = log_p_c_given_w + log_p_w\n",
    "        probabilities.append((candidate, op, log_p_w, log_p_c_given_w, log_p_w_given_c))\n",
    "    \n",
    "    # Sort by combined log probability\n",
    "    probabilities.sort(key=lambda x: x[-1], reverse=True)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\n",
    "probabilities = compute_correction_probabilities_log(word, candidates, error_model, language_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2568843200.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    candidate, op, log_p_w, log_p_c_given_w, log_p_w_given_c = row\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "if word in vocabulary:\n",
    "    print(f\"'{word}' is already correctly spelled.\")\n",
    "else:\n",
    "    header = \"{:<15} {:<15} {:<10} {:<20} {:<15} {:<15} {:<15}\"\n",
    "    print(header.format(\"word\", \"candidate\", \"edit_type\", \"edit\", \"log P(c)\", \"log P(w|c)\", \"P(c) x P(w|c)\"))\n",
    "    for row in probabilities:\n",
    "        candidate, op, log_p_w, log_p_c_given_w, log_p_w_given_c = row\n",
    "        char_edit_str = get_character_edit(word, candidate, op)\n",
    "        # Convert log probabilities back to normal scale for display\n",
    "        p_w = math.exp(log_p_w)\n",
    "        p_c_given_w = math.exp(log_p_c_given_w)\n",
    "        p_w_given_c = math.exp(log_p_w_given_c)\n",
    "        # Display more decimal places to avoid rounding to zero\n",
    "        print(header.format(word, candidate, op, char_edit_str, f\"{log_p_w:.6f}\", f\"{log_p_c_given_w:.6f}\", f\"{p_w_given_c:.10f}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
